---
layout: post
title: "Spark"
description: Spark
modified: 2022-01-19
category: BigData
tags: [BigData]
---

# 一、安装

    docker run -d --name=spark_single --privileged hdfs_proto /usr/sbin/init
    docker cp E:\software\bigdata\spark-3.1.2-bin-without-hadoop.tgz spark_single:/
    docker exec -it spark_single bash
    tar -zxf spark-3.1.2-bin-without-hadoop.tgz
    mv spark-3.1.2-bin-without-hadoop /usr/local/spark
    chown -R hadoop /usr/local/spark

    # 配置环境变量，退出docker容器并重新进入才生效！！
    echo "export SPARK_HOME=/usr/local/spark" >> /etc/bashrc
    echo "export PATH=$PATH:$SPARK_HOME/bin" >> /etc/bashrc

    cd /usr/local/spark/conf
    cp spark-env.sh.template spark-env.sh
    vim spark-env.sh
    # 添加如下
    export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)
    
    cd ~
    spark-shell
    :quit

    docker stop spark_single
    docker commit spark_single spark_proto

# 二、基础

1.执行过程

编写Spark应用与之前实现在Hadoop上的其他数据流语言类似。代码写入一个惰性求值的驱动程序（driver program）中，通过一个动作（action），驱动代码被分发到集群上，由各个RDD分区上的worker来执行。
然后结果会被发送回驱动程序进行聚合或编译。本质上，驱动程序创建一个或多个RDD，调用操作来转换RDD，然后调用动作处理被转换后的RDD。
定义一个或多个RDD，可以通过获取存储在磁盘上的数据（HDFS，Cassandra，HBase，Local Disk），并行化内存中的某些集合，转换（transform）一个已存在的RDD，或者，缓存或保存。
通过传递一个闭包（函数）给RDD上的每个元素来调用RDD上的操作。Spark提供了除了Map和Reduce的80多种高级操作。
使用结果RDD的动作（action）（如count、collect、save等）。动作将会启动集群上的计算。
当Spark在一个worker上运行闭包时，闭包中用到的所有变量都会被拷贝到节点上，但是由闭包的局部作用域来维护。
Spark提供了两种类型的共享变量，这些变量可以按照限定的方式被所有worker访问。广播变量会被分发给所有worker，但是是只读的。累加器这种变量，worker可以使用关联操作来“加”，通常用作计数器。

2.RDD操作

RDD转换：转换的作用是从现有数据集创建新数据集。转换是惰性的，因为它们仅在动作需要将结果返回到驱动程序时才计算。

    map(func) - 它返回一个新的分布式数据集， 该数据集是通过函数func传递源的每个元素而形成的。
    filter(func) - 它返回一个新数据集， 该数据集是通过选择函数func返回true的源元素而形成的。
    flatMap(func) - 这里，每个输入项可以映射到零个或多个输出项， 因此函数func应该返回序列而不是单个项。
    mapPartitions(func) - 它类似于map，但是在RDD的每个分区(块)上单独运行， 因此当在类型T的RDD上运行时， func必须是Iterator <T> => Iterator <U>类型。
    mapPartitionsWithIndex(func) - 它类似于mapPartitions，它为func提供了一个表示分区索引的整数值，因此当在类型T的RDD上运行时，func必须是类型(Int，Iterator <T>)=> Iterator <U>。
    sample(withReplacement, fraction, seed) - 它使用给定的随机数生成器种子对数据的分数部分进行采样，有或没有替换。
    union(otherDataset) - 它返回一个新数据集，其中包含源数据集和参数中元素的并集。
    intersection(otherDataset) - 它返回一个新的RDD，其中包含源数据集和参数中的元素的交集。
    distinct([numPartitions])) - 它返回一个新数据集，其中包含源数据集的不同元素。
    groupByKey([numPartitions]) - 它接收键值对(K，V)作为输入，基于键对值进行分组，并生成(K，Iterable)对的数据集作为输出。
    reduceByKey(func, [numPartitions]) - 当调用(K，V)对的数据集时，返回(K，V)对的数据集，其中使用给定的reduce函数func聚合每个键的值，该函数必须是类型(V，V)=>V。
    aggregateByKey(zeroValue)(seqOp, combOp, [numPartitions]) - 当调用(K，V)对的数据集时，返回(K，U)对的数据集，其中使用给定的组合函数和中性“零”值聚合每个键的值。
    sortByKey([ascending], [numPartitions]) - 它接收键值对(K，V)作为输入，按升序或降序对元素进行排序，并按顺序生成数据集。
    join(otherDataset, [numPartitions])-当调用类型(K，V)和(K，W)的数据集时，返回(K，(V，W))对的数据集以及每个键的所有元素对。通过leftOuterJoin，rightOuterJoin和fullOuterJoin支持外连接。
    cogroup(otherDataset, [numPartitions])-当调用类型(K，V)和(K，W)的数据集时，返回(K，(Iterable，Iterable))元组的数据集。此操作也称为groupWith。
    cartesian(otherDataset)-生成两个数据集的笛卡尔积，并返回所有可能的对组合。
    pipe(command, [envVars])-通过shell命令管道RDD的每个分区，例如， 一个Perl或bash脚本。
    coalesce(numPartitions)-它将RDD中的分区数减少到numPartitions。
    repartition(numPartitions) -它随机重新调整RDD中的数据，以创建更多或更少的分区，并在它们之间进行平衡。
    repartitionAndSortWithinPartitions(partitioner) - 它根据给定的分区器对RDD进行重新分区，并在每个生成的分区中键对记录进行排序。

RDD动作：动作的作用是在对数据集运行计算后将值返回给驱动程序。

    reduce(func)它使用函数func(它接受两个参数并返回一个)来聚合数据集的元素。该函数应该是可交换的和关联的，以便可以并行正确计算。
    collect()它将数据集的所有元素作为数组返回到驱动程序中。在过滤器或其他返回足够小的数据子集的操作之后，这通常很有用。
    count()它返回数据集中的元素数。
    first()它返回数据集的第一个元素(类似于take(1))。
    take(n)它返回一个包含数据集的前n个元素的数组。
    takeSample(withReplacement, num, [seed])它返回一个数组，其中包含数据集的num个元素的随机样本，有或没有替换，可选地预先指定随机数生成器种子。
    takeOrdered(n, [ordering])它使用自然顺序或自定义比较器返回RDD的前n个元素。
    saveAsTextFile(path)它用于将数据集的元素作为文本文件(或文本文件集)写入本地文件系统，HDFS或任何其他Hadoop支持的文件系统的给定目录中。
    saveAsSequenceFile(path)它用于在本地文件系统，HDFS或任何其他Hadoop支持的文件系统中的给定路径中将数据集的元素编写为Hadoop SequenceFile。
    saveAsObjectFile(path)它用于使用Java序列化以简单格式编写数据集的元素，然后可以使用SparkContext.objectFile()加载。
    countByKey()它仅适用于类型(K，V)的RDD。因此，它返回(K，Int)对的散列映射与每个键的计数。
    foreach(func)它在数据集的每个元素上运行函数func以获得副作用，例如更新累加器或与外部存储系统交互。

3.RDD持久化

Spark通过在操作中将其持久保存在内存中，提供了一种处理数据集的便捷方式。在持久化RDD的同时，每个节点都存储它在内存中计算的任何分区。也可以在该数据集的其他任务中重用它们。
我们可以使用persist()或cache()方法来标记要保留的RDD。Spark的缓存是容错的。在任何情况下，如果RDD的分区丢失，它将使用最初创建它的转换自动重新计算。
存在可用于存储持久RDD的不同存储级别。通过将StorageLevel对象(Scala，Java，Python)传递给persist()来使用这些级别。但是，cache()方法用于默认存储级别，即StorageLevel.MEMORY_ONLY。

    MEMORY_ONLY 它将RDD存储为JVM中的反序列化Java对象。这是默认级别。如果RDD不适合内存，则每次需要时都不会缓存和重新计算某些分区。
    MEMORY_AND_DISK 它将RDD存储为JVM中的反序列化Java对象。如果RDD不适合内存，请存储不适合的分区到磁盘，并在需要时从那里读取它们。
    MEMORY_ONLY_SER 它将RDD存储为序列化Java对象(即每个分区一个字节的数组)。这通常比反序列化的对象更节省空间。
    MEMORY_AND_DISK_SER 它类似于MEMORY_ONLY_SER，但是将内存中不适合的分区溢出到磁盘而不是重新计算它们。
    DISK_ONLY 它仅将RDD分区存储在磁盘上。
    MEMORY_ONLY_2, MEMORY_AND_DISK_2 它与上面的级别相同，但复制两个群集节点上的每个分区。
    OFF_HEAP 它类似于MEMORY_ONLY_SER，但将数据存储在堆外内存中。必须启用堆外内存。

4.RDD共享变量

在Spark中，当任何函数传递给转换操作时，它将在远程集群节点上执行。它适用于函数中使用的所有变量的不同副本。这些变量将复制到每台计算机，并且远程计算机上的变量更新不会恢复到驱动程序。

广播变量：
广播变量支持在每台机器上缓存的只读变量，而不是提供任务的副本。Spark使用广播算法来分发广播变量以降低通信成本。
spark动作的执行经过几个阶段，由分布式“shuffle”操作分开。Spark自动广播每个阶段中任务所需的公共数据。以这种方式广播的数据以序列化形式缓存并在运行每个任务之前反序列化。
要创建广播变量(比方说，v)，请调用SparkContext.broadcast(v)。

val a=sc.broadcast(Array(1,2,3))
a.value

累加器：
累加器是用于执行关联和交换操作(例如计数器或总和)的变量。Spark为数字类型的累加器提供支持。但是，可以添加对新类型的支持。
要创建数字累加器，请调用SparkContext.longAccumulator()或SparkContext.doubleAccumulator()以累积Long或Double类型的值。

val a=sc.longAccumulator("Accumulator")
sc.parallelize(Array(2,5)).foreach(x=>a.add(x))
a.value

# 三、实例

1.单词统计

    hadoop fs -put ~/hello.txt /user/hadoop/

    spark-shell
    # 以下为scala代码
    val data=sc.textFile("hello.txt")
    data.collect;
    val splitdata = data.flatMap(line => line.split(" "));
    splitdata.collect;
    val mapdata = splitdata.map(word => (word,1));
    mapdata.collect;
    val reducedata = mapdata.reduceByKey(_+_);
    # 等价于reduceByKey((x,y) => x+y)
    reducedata.collect;

# 参考：

1.[易百Spark教程](https://www.yiibai.com/spark)

2.[w3cschool Spark编程指南](https://www.w3cschool.cn/spark/)

3.[w3cschool spark sql](https://www.w3cschool.cn/spark_sql/)

4.[RDD Programming Guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html)

5.[Spark编程指南简体中文版](https://endymecy.gitbooks.io/spark-programming-guide-zh-cn/content/)

6.[子雨大数据之Spark入门教程(Python版)](http://dblab.xmu.edu.cn/blog/1709-2/)
